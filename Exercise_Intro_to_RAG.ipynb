{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://conted.ox.ac.uk/www/static/images/oudce_logo.svg\" width=500> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:navy\"><b>Artificial Intelligence: Generative AI, Cloud and MLOps (online)</b></span>\n",
    "---\n",
    "# <span style=\"color:#ff8000\">Introduction to Retrieval Augmented Generation (RAG)</span>\n",
    "01 March 2025\n",
    "\n",
    "Abhinav Kimothi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval Augmented Generation, or RAG, has emerged to be one of the most popular techniques in the applied generative AI world. Large Language Models, or LLMs, is a generative AI technology that has recently gained tremendous popularity. However, despite their unprecedented ability to generate text, their responses are __not always correct__. Upon more careful observation, you may notice that LLM responses are plagued with __sub-optimal information__ and __inherent memory limitations__.  RAG addresses these limitations of LLMs by providing them with information __external__ to these models. Thereby, resulting in LLM responses that are more reliable and trustworthy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Assets/Images/RAG.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The technique of __enhancing the parametric memory__ of an LLM by creating access to an __explicit non-parametric memory__, from which a __retriever__ can fetch relevant information, augment that information to the prompt, pass the prompt to an LLM to enable the LLM to generate a response that is __contextual, reliable, and factually accurate__ is called __Retrieval Augmented Generation__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an in-depth understanding of RAG, read [A Simple Guide to Retrieval Augemnted Generation](https://mng.bz/6ePo)\n",
    "\n",
    "<a href=\"https://mng.bz/6ePo\" target=\"_blank\">\n",
    "    <img src=\"./Assets/Images/NewMEAP.png\" width=350> <img src=\"./Assets/Images/Oxford University - kimothi.png\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff8000\">About this notebook</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a supplementary notebook for the session __Introduction to Retrieval Augmented Generation__ by _Abhinav Kimothi_ in week 7 of the __Artificial Intelligence: Generative AI, Cloud and MLOps (online)__ course by _Department for Continuing Education at the University of Oxford_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#47c7fc\">Contents</span>\n",
    "\n",
    "This notebook contains code in python and leverages the LangChain framework to build and evaluate the different components of a RAG pipeline. \n",
    "\n",
    "- Indexing Pipeline\n",
    "    -  Data Loading\n",
    "    - Chunking (or Data Splitting)\n",
    "    - Embeddings (or Data Transformation)\n",
    "    - Storage (Vector Databases)\n",
    "\n",
    "- Generation Pipeline\n",
    "    - Search & Retrieval\n",
    "    - Prompt Augmentation\n",
    "    - LLM Generation\n",
    "\n",
    "- RAG Evaluation using RAGAs Framework\n",
    "    - Synthetic Dataset Generation\n",
    "    - Calculation of Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#47c7fc\">Structure</span>\n",
    "\n",
    "Each section of this notebook first __demonstrates__ the components using an example and is followed by an __exercise for you to solve__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Demonstration__\n",
    "\n",
    "This notebook demonstrates __RAG using a webpage on the internet__. We know that LLMs inherently __do not have access to the internet__ and have a __knowledge cut-off date__ that prevents them from having access to latest information. RAG with websearch overcomes this limitation.\n",
    "\n",
    "In this notebook, we take the example of the __Wikipedia Article on 2023 Men's ODI Cricket World Cup__\n",
    "\n",
    "#### __Exercise__\n",
    "\n",
    "LLMs also do not have access to any data that is not in their training set. Therefore, __proprietary data is not available__ to LLMs. This where RAG helps in searching through proprietary data files. \n",
    "\n",
    "In this notebook, you'll be asked to build a RAG system on a PDF file which is neither available on the internet, nor is a part of any LLMs training dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope you have as much fun going through this notebook, as I had while creating it. Let's get started!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Important Note: This notebook requires OpenAI credits. The notebook uses __Text-embedding-3-small__ embeddings model and __GPT-4o-mini__ LLM. \n",
    "\n",
    "> __Running the entire notebook once including the exercises will cost about USD $0.11 (11 cents)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff8000\">Installing Dependencies</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the necessary libraries for running this notebook along with their versions can be found in __requirements.txt__ file in the root directory of this repository\n",
    "\n",
    "You should go to the root directory and run the following command to install the libraries\n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "This is the recommended method of installing the dependencies\n",
    "\n",
    "\n",
    "_Alternatively, you can run the command from this notebook too. The relative path may vary so ensure that you are in the root directory of this repository_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ./requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff8000\">Indexing Pipeline</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A core RAG system contains two pipelines - \n",
    " - Indexing Pipeline that creates the external knowledge base \n",
    " - Generation Pipeline that facilitates real-time interaction with the knowledge base\n",
    "\n",
    "The indexing pipeline can be understood in five steps\n",
    "\n",
    "<img src=\"./Assets/Images/Indexing Pipeline.png\" width=500>\n",
    "\n",
    "There are consequently __four components__ that facilitate these five steps - \n",
    "\n",
    "<img src=\"./Assets/Images/Index Comp.png\" width = 500>\n",
    "\n",
    "Let's take a look at these four components\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#47c7fc\">Data Loading</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __What is Data Loading?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step towards building a knowledge base (or non-parametric memory) of a RAG-enabled system is to source data from its original location. This data may be in the form of word documents, pdf files, csv, HTML etc. Further, the data may be stored in file, block or object stores, in data lakes, data warehouses or even in third party sources that can be accessed via the open internet. This process of sourcing data from its original location is called __Data Loading__. \n",
    "\n",
    "Data Loading includes the following four steps:\n",
    "- __Connection__ to the source of the data\n",
    "- __Extraction and Parsing of text__ from the source format\n",
    "- Reviewing and updating __metadata__ information\n",
    "- Cleaning or __transforming__ the data\n",
    "\n",
    "<img src=\"./Assets/Images/DataLoading.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Connecting & Parsing an external URL__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load the url of our example i.e. the Wikipedia Page of the 2023 Cricket World Cup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url of the wikipedia page on the 2023 Cricket World Cup\n",
    "url=\"https://en.wikipedia.org/wiki/2023_Cricket_World_Cup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__LangChain__ provides a wide array of __document loaders__ (over 100) that help in loading data from a large number of data sources like _Webpages, PDF files, Cloud Storage Systems, Social Platforms, Messaging Services, Tools and more_. \n",
    "\n",
    "Here we use one of them, __AsyncHtmlLoader__ that loads the HTML data from web URLs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import library\n",
    "from langchain_community.document_loaders import AsyncHtmlLoader\n",
    "\n",
    "#Instantiate the AsyncHtmlLoader object\n",
    "loader = AsyncHtmlLoader (url)\n",
    "\n",
    "#Loading the extracted information\n",
    "html_data = loader.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the extracted text and the metadata, let us print a few tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "print(textwrap.fill(f\"First 1000 characters of extracted content -\\n\\n{html_data[0].page_content[:1000]}\", width=150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Metadata Review__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Metadata information - \\n\\n{html_data[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some __content has been extracted__. Also, some __metadata__ information is present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Document Transformation__\n",
    "\n",
    "The content is in __HTML format__ which does not convey a lot of factual information. \n",
    "\n",
    "LangChain also provides a bunch of document transformers for converting formats.\n",
    "\n",
    "\n",
    "We will now transform this data into a readable format using __Html2TextTransformer__ class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_transformers import Html2TextTransformer\n",
    "\n",
    "#Instantiate the Html2TextTransformer function\n",
    "html2text = Html2TextTransformer()\n",
    "\n",
    "\n",
    "#Call transform_documents\n",
    "html_data_transformed = html2text.transform_documents(html_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us review the extracted content, now transformed by the Html2TextTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"First 100 characters of extracted content -\\n\\n{html_data_transformed[0].page_content[:1000]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we see that we have text in a __readable english__ language! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Optional: BeautifulSoupTransformer__\n",
    "\n",
    "\n",
    "But you may notics that there's a lot of information like Menu Options, Header and footer information that may not be very useful.\n",
    "\n",
    "Another options is the __BeautifulSoupTransformer__ in LangChain that allows you to extract specific tags from HTML pages. Let us extract information contained in 'p' tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_transformers import BeautifulSoupTransformer\n",
    "\n",
    "soup_transformer = BeautifulSoupTransformer()\n",
    "\n",
    "html_data_p_tags = soup_transformer.transform_documents(html_data, tags_to_extract=[\"p\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(textwrap.fill(\n",
    "f\"First 100 characters of extracted content -\\n\\n{html_data_p_tags[0].page_content[:1000]}\", width=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen how to load text from an external source. \n",
    "\n",
    "Now it's time for you to try data loading!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#47c7fc\">Exercise: PDF Document (Employee Leave Policy)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file that you're going to read is the __Employee Leave Policy__ of a fictitious organisation named AKAIWorks LLP. The file is present in a __PDF format__ and the folder location is __'./Assets/Data/'__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath='./Assets/Data/EmployeeLeavePolicy.pdf'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Exercise:__\n",
    "\n",
    "Your task is to load the pdf file using the PyPDFLoader and print the first 1000 characters of the text that has been read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "#START YOUR CODE HERE\n",
    "\n",
    "\n",
    "#END YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click for Hint</summary>\n",
    "\n",
    "Checkout the [PyPDFLoader documentation](https://python.langchain.com/docs/integrations/document_loaders/pypdfloader/#extract-the-whole-pdf-as-a-single-langchain-document-object)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click for solution</summary>\n",
    "\n",
    "```\n",
    "pdfloader=PyPDFLoader(file_path=filepath, mode='single') #instantiate the PyPDFLoader\n",
    "\n",
    "pdf_data=pdfloader.load() #load the data\n",
    "\n",
    "print(textwrap.fill(f\"{pdf_data[0].page_content[:1000]}\", width=150)) #print the first 1000 characters\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Congratulations__\n",
    "\n",
    "With this, you have successfully completed the data loading step of the indexing pipeline. We move now to the next step of __Chunking__\n",
    "\n",
    "But before that, check out the document loaders and transformers available in LangChain\n",
    "\n",
    "__Document Loaders__  - [https://python.langchain.com/docs/integrations/document_loaders/]\n",
    "\n",
    "__Document Transformers__ - [https://python.langchain.com/docs/integrations/document_transformers/]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff8000\">2. Data Splitting or Chunking</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaking down long pieces of text into manageable sizes is called __Data Splitting__ or __Chunking__. This is done for various reasons like Context Window Limitations, Search Complexity, Lost in the middle kind of issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#47c7fc\">Understanding Chunking: What is it ?</span>\n",
    "\n",
    "In cognitive psychology, chunking is defined as process by which individual pieces of information are bound together into a meaningful whole. (https://psycnet.apa.org/record/2003-09163-002) and a chunk is a familiar collection of elementary units. The idea is that chunking is an essential technique through which human beings perceive the world and commit to memory. The simplest example is how we remember long sequences of digits like phone numbers, credit card numbers, dates or even OTPs. We don’t remember the entire sequences but in our minds, we break them down into chunks.\n",
    "\n",
    "The role of chunking in RAG and the underlying idea is somewhat similar to what it is in real life. Once you’ve extracted and parsed text from the source, instead of committing it all to memory as a single element, you break it down into smaller chunks.\n",
    "\n",
    "> Breaking down long pieces of text into manageable sizes is called Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#47c7fc\">Understanding Chunking: Why is it necessary ?</span>\n",
    "There are two main benefits of chunking —\n",
    "\n",
    "- It leads to better retrieval of information. If a chunk represents a single idea (or fact) it can be retrieved with more confidence that if there are multiple ideas (or facts) within the same chunk.\n",
    "- It leads to better generation. The retrieved chunk has information that is focussed on the user query and does not have any other text that may confuse the LLM. Therefore, the generation is more accurate and coherent.\n",
    "\n",
    "Apart from these two benefits there are two limitations of LLMs that chunking addresses.\n",
    "\n",
    "- __Context Window of LLMs__: LLMs, due to the inherent nature of the technology, have a limit on the number of tokens (loosely, words) they can work with at a time. This includes both the number of tokens in the prompt (or the input) and the number of tokens in the completion (or the output). The limit on the total number of tokens that an LLM can process in one go is called the context window size. If we pass an input that is longer than the context window size, the LLM chooses to ignore all text beyond the size. It becomes very important to be careful with the amount to text that is being passed to the LLM.\n",
    "\n",
    "- __Lost in the middle problem__: Even in those LLMs which have a long context window (Claude 3 by Anthropic has a context window of up to 200,00 tokens), an issue with accurately reading the information has been observed. It has been noticed that accuracy declines dramatically if the relevant information is somewhere in the middle of the prompt. This problem can be addressed by passing only the relevant information to the LLM instead of the entire document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#47c7fc\">Fixed Size Chunking</span>\n",
    "\n",
    "A very common approach is to pre-determine the size of the chunk and the amount of overlap between the chunks. There are several chunking methods that follow a fixed size chunking approach.\n",
    "\n",
    "- Character-Based Chunking: Chunks are created based on a fixed number of characters\n",
    "\n",
    "- Token-Based Chunking: Chunks are created based on a fixed number of tokens.\n",
    "\n",
    "- Sentence-Based Chunking: Chunks are defined by a fixed number of sentences\n",
    "\n",
    "- Paragraph-Based Chunking: Chunks are created by dividing the text into a fixed number of paragraphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try Character-Based Chunking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter #Character Based Text Splitter from LangChain\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "separators=[\"\\n\",\".\"], #The character that should be used to split. More than one can be given to try recursively.\n",
    "chunk_size=1000, #Number of characters in each chunk \n",
    "chunk_overlap=100, #Number of overlapping characters between chunks\n",
    ")\n",
    "\n",
    "text_chunks=text_splitter.create_documents([html_data_transformed[0].page_content])\n",
    "\n",
    "#Show the number of chunks created\n",
    "print(f\"The number of chunks created : {len(text_chunks)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see the size distribution of the chunks that have been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data = [len(doc.page_content) for doc in text_chunks]\n",
    "\n",
    "plt.boxplot(data)  \n",
    "plt.title('Box Plot of chunk lengths') \n",
    "plt.xlabel('Chunk Lengths')  \n",
    "plt.ylabel('Values') \n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"The median chunk length is : {round(np.median(data),2)}\")\n",
    "print(f\"The average chunk length is : {round(np.mean(data),2)}\")\n",
    "print(f\"The minimum chunk length is : {round(np.min(data),2)}\")\n",
    "print(f\"The max chunk length is : {round(np.max(data),2)}\")\n",
    "print(f\"The 75th percentile chunk length is : {round(np.percentile(data, 75),2)}\")\n",
    "print(f\"The 25th percentile chunk length is : {round(np.percentile(data, 25),2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#47c7fc\">Document-structured based Chunking</span>\n",
    "\n",
    "The aim of chunking is to keep meaningful data together. If we are dealing with data in form of HTML, Markdown, JSON or even computer code, it makes more sense to split the data based on the structure rather than a fixed size. Another approach for chunking is to take into consideration the format of the extracted and loaded data. A markdown file, for example is organised by headers, a code written in a programming language like python or java is organized by classes and functions and HTML, likewise, is organised in headers and sections. For such formats a specialised chunking approach can be employed.\n",
    "\n",
    "Examples of structure-based splitting:\n",
    "\n",
    "- Markdown: Split based on headers (e.g., #, ##, ###)\n",
    "- HTML: Split using tags\n",
    "- JSON: Split by object or array elements\n",
    "- Code: Split by functions, classes, or logical blocks\n",
    "\n",
    "\n",
    "Let's recollect out HTML document from the url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = AsyncHtmlLoader (url)\n",
    "\n",
    "html_data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To split the HTML text based on tags (e.g., h1, section, table, etc.), LangChain provides __HTMLSectionSplitter__. It splits the text and adds metadata for each section. Let's take a look.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import HTMLSectionSplitter\n",
    "\n",
    "sections_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "     (\"h2\", \"Header 2\"),\n",
    "     (\"table\",\"Table\"),\n",
    "     #(\"div\", \"Div\"),\n",
    "     #(\"img\",\"Image\"),\n",
    "     (\"p\",\"P\"),\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "splitter = HTMLSectionSplitter(sections_to_split_on)\n",
    "\n",
    "split_content = splitter.split_text(html_data[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above document object '__split_content__' will have chunks divided based on the provided HTML tags. Let's look at the top 10 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_content[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the metatadata indicating the section tag of the chunk.\n",
    "So how many chunks were created?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(split_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many chunks for each of the sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class_counter = Counter()\n",
    "\n",
    "for doc in split_content:\n",
    "    document_class = next(iter(doc.metadata.keys()))\n",
    "    class_counter[document_class] += 1\n",
    "\n",
    "print(class_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us look at the lengths of these chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = [len(doc.page_content) for doc in split_content]\n",
    "\n",
    "plt.boxplot(data)  \n",
    "plt.title('Box Plot of chunk lengths')\n",
    "plt.xlabel('Chunk Lengths')  \n",
    "plt.ylabel('Values')  \n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"The median chunk lenght is : {round(np.median(data),2)}\")\n",
    "print(f\"The average chunk lenght is : {round(np.mean(data),2)}\")\n",
    "print(f\"The minimum chunk lenght is : {round(np.min(data),2)}\")\n",
    "print(f\"The max chunk lenght is : {round(np.max(data),2)}\")\n",
    "print(f\"The 75th percentile chunk length is : {round(np.percentile(data, 75),2)}\")\n",
    "print(f\"The 25th percentile chunk length is : {round(np.percentile(data, 25),2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the chunk lengths are longer than 1000. Let's try to control that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "separators=[\"\\n\\n\",\"\\n\",\".\"], #The character that should be used to split. More than one can be given to try recursively.\n",
    "chunk_size=1000, #Number of characters in each chunk \n",
    "chunk_overlap=100, #Number of overlapping characters between chunks\n",
    ")\n",
    "\n",
    "final_chunks=text_splitter.split_documents(split_content)\n",
    "\n",
    "#Show the number of chunks created\n",
    "print(f\"The number of chunks created : {len(final_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = [len(doc.page_content) for doc in final_chunks]\n",
    "\n",
    "plt.boxplot(data)  \n",
    "plt.title('Box Plot of chunk lengths')  # Title\n",
    "plt.xlabel('Chunk Lengths')  # Label for x-axis\n",
    "plt.ylabel('Values')  # Label for y-axis\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"The median chunk lenght is : {round(np.median(data),2)}\")\n",
    "print(f\"The average chunk lenght is : {round(np.mean(data),2)}\")\n",
    "print(f\"The minimum chunk lenght is : {round(np.min(data),2)}\")\n",
    "print(f\"The max chunk lenght is : {round(np.max(data),2)}\")\n",
    "print(f\"The 75th percentile chunk length is : {round(np.percentile(data, 75),2)}\")\n",
    "print(f\"The 25th percentile chunk length is : {round(np.percentile(data, 25),2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There, we have our final chunks! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#47c7fc\">Exercise: PDF Document (Employee Leave Policy)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous exercise you read the __Employee Leave Policy__ of AKAIWorks LLP. Now, you should use the recursive character splitter to create chunks of this document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfloader=PyPDFLoader(file_path=filepath,mode=\"single\") #instantiate the PyPDFLoader\n",
    "\n",
    "pdf_data=pdfloader.load() #load the data\n",
    "\n",
    "print(len(pdf_data[0].page_content))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to now chunk this document into manageable sizes using __RecursiveCharacterTextSplitter__ and store in document object ```pdf_doc_chunks``` and print the number of chunks created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#START YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "#END YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click for Hint</summary>\n",
    "\n",
    "Checkout the [RecursiveCharacterTextSplitter documentation](https://python.langchain.com/docs/how_to/recursive_text_splitter/)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click for Solution</summary>\n",
    "\n",
    "```\n",
    "text_splitter =RecursiveCharacterTextSplitter(\n",
    "separators=[\"\\n\\n\",\"\\n\",\".\"], #The character that should be used to split. More than one can be given to try recursively.\n",
    "chunk_size=1000, #Number of characters in each chunk \n",
    "chunk_overlap=100, #Number of overlapping characters between chunks\n",
    ")\n",
    "\n",
    "pdf_doc_chunks=text_splitter.split_documents(pdf_data)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the distribution of chunk sizes.\n",
    "\n",
    "Run the cell below.\n",
    "\n",
    "Remember the document object should be called ```pdf_doc_chunks```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [len(doc.page_content) for doc in pdf_doc_chunks]\n",
    "\n",
    "plt.boxplot(data)  \n",
    "plt.title('Box Plot of chunk lengths')  # Title \n",
    "plt.xlabel('Chunk Lengths')  # Label for x-axis\n",
    "plt.ylabel('Values')  # Label for y-axis\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"The median chunk lenght is : {round(np.median(data),2)}\")\n",
    "print(f\"The average chunk lenght is : {round(np.mean(data),2)}\")\n",
    "print(f\"The minimum chunk lenght is : {round(np.min(data),2)}\")\n",
    "print(f\"The max chunk lenght is : {round(np.max(data),2)}\")\n",
    "print(f\"The 75th percentile chunk length is : {round(np.percentile(data, 75),2)}\")\n",
    "print(f\"The 25th percentile chunk length is : {round(np.percentile(data, 25),2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Congratulations__\n",
    "\n",
    "With this, you have successfully completed the chunking of the data. We move now to the next step of creating __Embeddings__\n",
    "\n",
    "But before that, check out the splitters available in LangChain\n",
    "\n",
    "__Text Splitters__  - [https://python.langchain.com/docs/concepts/text_splitters/]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff8000\">3. Data Transformation or Embeddings</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computers, at the very core, do mathematical calculations. Mathematical calculations are done on numbers. Therefore, for a computer to process any kind of non-numeric data like text or image, it must be first converted into a numerical form. \n",
    "\n",
    "Embeddings is a design pattern that is extremely helpful in the fields of data science, machine learning and artificial intelligence. Embeddings are vector representations of data. As a general definition, embeddings are data that has been transformed into n-dimensional matrices. A word embedding is a vector representation of words. \n",
    "\n",
    "<img src=\"./Assets/Images/Embeddings.png\" width=900>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Open Source Embeddings from HuggingFace__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with an opensource embeddings from HuggingFace!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "hf_embeddings = embeddings.embed_documents([chunk.page_content for chunk in final_chunks])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The lenght of the embeddings vector is {len(hf_embeddings[0])}\")\n",
    "print(f\"The embeddings object is an array of {len(hf_embeddings)} X {len(hf_embeddings[0])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __OpenAI Embeddings__\n",
    "\n",
    "OpenAI, the company behind ChatGPT and GPT series of Large Language Models also provide three Embeddings Models. \n",
    "\n",
    "1.\ttext-embedding-ada-002 was released in December 2022. It has a dimension of 1536 meaning that it converts text into a vector of 1536 dimensions.\n",
    "2.\ttext-embedding-3-small is the latest small embedding model of 1536 dimensions released in January 2024. The flexibility it provides over ada-002 model is that users can adjust the size of the dimensions according to their needs.\n",
    "3.\ttext-embedding-3-large is a large embedding model of 3072 dimensions released together with the text-embedding-3-small model. It is the best performing model released by OpenAI yet.\n",
    "\n",
    "\n",
    "OpenAI models are proprietary and can be accessed using the OpenAI API and are priced based on the number of input tokens for which embeddings are desired. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You will need an __OpenAI API Key__ which can be obtained from [OpenAI](https://platform.openai.com/api-keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To initialize the __OpenAI client__, we need to pass the api key. There are many ways of doing it. \n",
    "\n",
    "__[Option 1] Creating a .env file for storing the API key and using it # Recommended__\n",
    "\n",
    "Install the __dotenv__ library\n",
    "\n",
    "_The dotenv library is a popular tool used in various programming languages, including Python and Node.js, to manage environment variables in development and deployment environments. It allows developers to load environment variables from a .env file into their application's environment._\n",
    "\n",
    "- Create a file named .env in the root directory of their project.\n",
    "- Inside the .env file, then define environment variables in the format VARIABLE_NAME=value. \n",
    "\n",
    "e.g.\n",
    "\n",
    "OPENAI_API_KEY=YOUR API KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "if load_dotenv():\n",
    "    print(\"Success: .env file found with some environment variables\")\n",
    "else:\n",
    "    print(\"Caution: No environment variables found. Please create .env file in the root directory or add environment variables in the .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[Option 2] Alternatively, you can set the API key in code.__\n",
    "\n",
    "However, this is not recommended since it can leave your key exposed for potential misuse. Uncomment the cell below to use this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-xxxxxxxxxxxxx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also test if the key is valid or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "if api_key:\n",
    "    try:\n",
    "        client.models.list()\n",
    "        print(\"OPENAI_API_KEY is set and is valid\")\n",
    "    except openai.APIError as e:\n",
    "        print(f\"OpenAI API returned an API Error: {e}\")\n",
    "        pass\n",
    "    except openai.APIConnectionError as e:\n",
    "        print(f\"Failed to connect to OpenAI API: {e}\")\n",
    "        pass\n",
    "    except openai.RateLimitError as e:\n",
    "        print(f\"OpenAI API request exceeded rate limit: {e}\")\n",
    "        pass\n",
    "\n",
    "else:\n",
    "    print(\"Please set you OpenAI API key as an environment variable OPENAI_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the __OpenAIEmbeddings__ library from langchain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OpenAIEmbeddings from the library\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]=\"false\"\n",
    "\n",
    "# Instantiate the embeddings object\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Create embeddings for all chunks\n",
    "openai_embeddings = embeddings.embed_documents([chunk.page_content for chunk in final_chunks])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The lenght of the embeddings vector is {len(openai_embeddings[0])}\")\n",
    "print(f\"The embeddings object is an array of {len(openai_embeddings)} X {len(openai_embeddings[0])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#47c7fc\">Exercise: PDF Document (Employee Leave Policy)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the chunks that you created in the previous section you will need to convert them into embeddings. \n",
    "\n",
    "Use __text-embedding-3-small__ embeddings using __OpenAIEmbeddings__ from langchain. Store the embeddings in ```pdf_doc_embeddings```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#START YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#END YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click for Hint</summary>\n",
    "\n",
    "Checkout the [OpenAIEmbeddings documentation](https://python.langchain.com/docs/integrations/text_embedding/openai/)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click for Solution</summary>\n",
    "\n",
    "```\n",
    "pdf_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "pdf_doc_embeddings=embeddings.embed_documents([chunk.page_content for chunk in pdf_doc_chunks])\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The lenght of the embeddings vector is {len(pdf_doc_embeddings[0])}\")\n",
    "print(f\"The embeddings object is an array of {len(pdf_doc_embeddings)} X {len(pdf_doc_embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Congratulations__\n",
    "\n",
    "With this, you have successfully completed the creation of embeddings. We move now to the next step of storing the embeddings in a  __Vector Store__\n",
    "\n",
    "Read more about [Embedding Models Here](https://python.langchain.com/docs/integrations/text_embedding/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff8000\">4. Vector Storage</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has been loaded, split, and converted into embeddings. For us to use this information repeatedly, we need to store it in memory so that it can be accessed on demand. Vector Databases are built to handle high dimensional vectors. These databases specialize in indexing and storing vector embeddings for fast semantic search and retrieval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "index = faiss.IndexFlatIP(len(openai_embeddings[0]))\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "\n",
    "vector_store.add_documents(documents=final_chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save the vector store in persistent memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.save_local(folder_path=\"./Memory\",index_name=\"CWC_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#47c7fc\">Exercise: PDF Document (Employee Leave Policy)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and store a __FAISS__ index. Use __IndexFlatIP__ \n",
    "\n",
    "Use __text-embedding-3-small__ embeddings using __OpenAIEmbeddings__ from langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_file_path=\"./Memory\"\n",
    "storage_index_name=\"PDF_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#START YOUR CODE HERE\n",
    "\n",
    "\n",
    "#END YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click for Hint</summary>\n",
    "\n",
    "Checkout the [FAISS documentation](https://python.langchain.com/docs/integrations/vectorstores/faiss/)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click for Solution</summary>\n",
    "\n",
    "```\n",
    "index = faiss.IndexFlatIP(len(pdf_doc_embeddings[0]))\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "\n",
    "vector_store.add_documents(documents=pdf_doc_chunks)\n",
    "\n",
    "vector_store.save_local(folder_path=storage_file_path,index_name=storage_index_name)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Congratulations__\n",
    "\n",
    "With this, you have successfully completed the creation the  __Vector Store__.\n",
    "\n",
    "The four steps of loading, chunking, embedding and storing complete the __indexing pipeline__. Indexing pipeline is an __offline process__. The Vector Index needs to be created once and then updated at a periodic frequency.\n",
    "\n",
    "Now, we will move on to the __generation pipeline__ and we will use this created index or knowledge base to handle real-time generations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff8000\">Generation Pipeline</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generation pipeline consists of three steps -\n",
    "\n",
    "1. Retrieval\n",
    "2. Augmentation\n",
    "3. Generation\n",
    "\n",
    "<img src=\"./Assets/Images/Generation Pipeline.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff8000\">1. Retrieval</span>\n",
    "\n",
    "We will now retrieve a relevant passage from the knowledge base that is pertinent to our query - __\"Who won the World Cup?\"__\n",
    "\n",
    "<img src=\"./Assets/Images/Retrieval.png\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FAISS vector store with safe deserialization\n",
    "vector_store = FAISS.load_local(folder_path=\"./Memory\",index_name=\"CWC_index\", embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Define a query\n",
    "query = \"Who won the world cup?\"\n",
    "\n",
    "# Perform similarity search\n",
    "retrieved_docs = vector_store.similarity_search(query, k=2)  # Get top 2 relevant chunks\n",
    "\n",
    "# Display results\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(textwrap.fill(f\"\\nRetrieved Chunk {i+1}:\\n{doc.page_content}\",width=100))\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the most basic implementation of a retriever in the generation pipeline of a RAG-enabled system. This method of retrieval is enabled by embeddings. We used the text-embedding-3-small from OpenAI. FAISS calculated the similarity score based on these embeddings.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff8000\">2. Augmentation</span>\n",
    "\n",
    "The information fetched by the retriever should also be sent to the LLM in form of a natural language prompt. This process of combining the user query and the retrieved information is called augmentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_context=retrieved_docs[0].page_content + retrieved_docs[1].page_content\n",
    "\n",
    "# Creating the prompt\n",
    "augmented_prompt=f\"\"\"\n",
    "\n",
    "Given the context below answer the question.\n",
    "\n",
    "Question: {query} \n",
    "\n",
    "Context : {retrieved_context}\n",
    "\n",
    "Remember to answer only based on the context provided and not from any other source. \n",
    "\n",
    "If the question cannot be answered based on the provided context, say I don’t know.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(textwrap.fill(augmented_prompt,width=150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff8000\">3. Generation</span>\n",
    "\n",
    "Generation is the final step of this pipeline. While LLMs may be used in any of the previous steps in the pipeline, the generation step is completely reliant on the LLM. The most popular LLMs are the ones being developed by OpenAI, Anthropic, Meta, Google, Microsoft and Mistral amongst other developers. \n",
    "\n",
    "We have built a simple retriever using FAISS and OpenAI embeddings and, we created a simple augmented prompt. Now we will use OpenAI’s model, GPT-4o-mini, to generate the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Set up LLM and embeddings\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")\n",
    "\n",
    "messages=[(\"human\",augmented_prompt)]\n",
    "\n",
    "ai_msg = llm.invoke(messages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_msg.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there you have it. The response is rooted in the HTML document and based on the chunks retrieved from the vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#47c7fc\">Exercise: PDF Document</span>\n",
    "\n",
    "Your exercise is to get an answer to the question - __How many paternity leaves can I avail?__\n",
    "\n",
    "The FAISS Index __PDF_index__ has already been created by you in the previous exercise. Now use __similarity_search__ and __ChatOpenAI__ library to get your answer.\n",
    "\n",
    "Begin with the loading the index and retrieve the chunks. Retrieve top 2 chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#START YOUR CODE HERE\n",
    "\n",
    "\n",
    "#END YOUR CODE HERE\n",
    "\n",
    "# Display results\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(textwrap.fill(f\"\\nRetrieved Chunk {i+1}:\\n{doc.page_content}\",width=100))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click for Solution</summary>\n",
    "\n",
    "```\n",
    "\n",
    "# Load the FAISS vector store with safe deserialization\n",
    "vector_store = FAISS.load_local(folder_path=\"./Memory\",index_name=\"PDF_index\", embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Define a query\n",
    "query = \"How many paternity leaves can I avail\"\n",
    "\n",
    "# Perform similarity search to get top 2 relevant chunks\n",
    "retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now craft the augmented prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#START YOUR CODE HERE\n",
    "\n",
    "\n",
    "#END YOUR CODE HERE\n",
    "\n",
    "print(textwrap.fill(augmented_prompt,width=150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click for Solution</summary>\n",
    "\n",
    "```\n",
    "\n",
    "retrieved_context=retrieved_docs[0].page_content + retrieved_docs[1].page_content\n",
    "\n",
    "# Creating the prompt\n",
    "augmented_prompt=f\"\"\"\n",
    "\n",
    "Given the context below answer the question.\n",
    "\n",
    "Question: {query} \n",
    "\n",
    "Context : {retrieved_context}\n",
    "\n",
    "Remember to answer only based on the context provided and not from any other source. \n",
    "\n",
    "If the question cannot be answered based on the provided context, say I don’t know.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, make the call to the LLM. Use OpenAI's __gpt-4o-mini__ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START YOUR CODE HERE\n",
    "\n",
    "\n",
    "# END YOUR CODE HERE\n",
    "\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click for Solution</summary>\n",
    "\n",
    "```\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None\n",
    ")\n",
    "\n",
    "messages=[(\"human\",augmented_prompt)]\n",
    "\n",
    "ai_msg = llm.invoke(messages)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Congratulations__\n",
    "\n",
    "With this, you have completed the construction of the core RAG pipeline!!! In the cell below you'll find all the above generation pipeline code in a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Replace non-breaking space with regular space\n",
    "    text = text.replace('\\xa0', ' ')\n",
    "    \n",
    "    # Remove any HTML tags (if any)\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # Removes HTML tags\n",
    "    \n",
    "    # Remove references in brackets (e.g., [7], [39])\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # Removes references inside square brackets\n",
    "    \n",
    "    # Remove extra spaces and newlines\n",
    "    text = ' '.join(text.split())  # This will remove extra spaces and newline characters\n",
    "    \n",
    "    return text\n",
    "\n",
    "def rag_function(query, db_path, index_name):\n",
    "    embeddings=OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "    db=FAISS.load_local(folder_path=db_path, index_name=index_name, embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "    retrieved_docs = db.similarity_search(query, k=2)\n",
    "\n",
    "    retrieved_context=[clean_text(retrieved_docs[0].page_content + retrieved_docs[1].page_content)]\n",
    "\n",
    "\n",
    "    augmented_prompt=f\"\"\"\n",
    "\n",
    "    Given the context below answer the question.\n",
    "\n",
    "    Question: {query} \n",
    "\n",
    "    Context : {retrieved_context}\n",
    "\n",
    "    Remember to answer only based on the context provided and not from any other source. \n",
    "\n",
    "    If the question cannot be answered based on the provided context, say I don’t know.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    "    )\n",
    "\n",
    "    messages=[(\"human\",augmented_prompt)]\n",
    "\n",
    "    ai_msg = llm.invoke(messages)\n",
    "\n",
    "    response=ai_msg.content\n",
    "\n",
    "    return retrieved_context, response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_function(query=\"How many paternity leaves can I avail?\", db_path=\"./Memory\", index_name=\"PDF_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the RAG system that we have created generating the responses on the expected lines? Is the LLM still hallucinating? Before trying to improve the performance of the system we need to be able to measure and benchmark it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff8000\">Evaluation</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ragas](https://docs.ragas.io/en/stable/) is a framework that helps you evaluate your Retrieval Augmented Generation (RAG) pipelines. It has been developed by the good folks at [exploding gradients](https://github.com/explodinggradients)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at this evaluation in 2 parts. \n",
    "\n",
    "1. Creation of synthetic test data for evaluation.\n",
    "2. Calculation of evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Creation of Synthetic Data\n",
    "\n",
    "Synthetic Data Generation uses LLMs to generate diverse questions and answers from the documents in the knowledge base. LLMs can be prompted to create questions like simple questions, multi-context questions, conditional questions, reasoning questions etc. using the documents from the knowledge base as context.\n",
    "\n",
    "<img src=\"./Assets/Images/SData.png\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
    "dataset = generator.generate_with_langchain_docs(html_data_transformed, testset_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_queries = dataset.to_pandas()['user_input'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_responses=dataset.to_pandas()['reference'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_to_eval=[]\n",
    "\n",
    "for query, reference in zip(sample_queries,expected_responses):\n",
    "    rag_call_response=rag_function(query=query, db_path=\"./Memory\", index_name=\"CWC_index\")\n",
    "    relevant_docs=rag_call_response[0]\n",
    "    response=rag_call_response[1]\n",
    "    dataset_to_eval.append(\n",
    "        {\n",
    "            \"user_input\":query,\n",
    "            \"retrieved_contexts\":relevant_docs,\n",
    "            \"response\":response,\n",
    "            \"reference\":reference\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset\n",
    "evaluation_dataset = EvaluationDataset.from_list(dataset_to_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, AnswerCorrectness, ResponseRelevancy\n",
    "\n",
    "result = evaluate(dataset=evaluation_dataset,metrics=[LLMContextRecall(), Faithfulness(), AnswerCorrectness(), ResponseRelevancy(), FactualCorrectness()],llm=evaluator_llm)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#47c7fc\">Exercise: PDF Document</span>\n",
    "\n",
    "In this final exercise, you need to generate a synthetic dataset using Ragas and then evaluate the responses of your RAG pipeline for the synthetically generated queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Complete the Code Below\n",
    "\n",
    "generator_llm = \"\"\n",
    "generator_embeddings = \"\"\n",
    "generator = \"\"\n",
    "dataset = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click for Solution</summary>\n",
    "\n",
    "\n",
    "```\n",
    "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
    "dataset = generator.generate_with_langchain_docs(pdf_data, testset_size=10)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_queries_pdf = dataset.to_pandas()['user_input'].to_list()\n",
    "expected_responses_pdf=dataset.to_pandas()['reference'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create the dataset to eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_to_eval_pdf=[]\n",
    "# START YOUR CODE\n",
    "\n",
    "\n",
    "# END YOUR CODE\n",
    "\n",
    "evaluation_dataset_pdf = EvaluationDataset.from_list(dataset_to_eval_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click for Solution</summary>\n",
    "\n",
    "```\n",
    "for query, reference in zip(sample_queries_pdf,expected_responses_pdf):\n",
    "    rag_call_response=rag_function(query=query, db_path=\"./Memory\", index_name=\"PDF_index\")\n",
    "    relevant_docs=rag_call_response[0]\n",
    "    response=rag_call_response[1]\n",
    "    dataset_to_eval_pdf.append(\n",
    "        {\n",
    "            \"user_input\":query,\n",
    "            \"retrieved_contexts\":relevant_docs,\n",
    "            \"response\":response,\n",
    "            \"reference\":reference\n",
    "        }\n",
    "    )\n",
    "\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we calculate a few evaluation metrics - __ContextRecall, Faithfulness, FactualCorrectness, AnswerCorrectness & ResponseRelevancy__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = evaluate(dataset=evaluation_dataset_pdf,metrics=[LLMContextRecall(), Faithfulness(), AnswerCorrectness(), ResponseRelevancy(), FactualCorrectness()],llm=evaluator_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff8000\">Congratulations!</span>\n",
    "For completing this introduction to RAG. I hope you had fun. For any queries, please get in touch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Assets/Images/profile_s.png\" width=100> \n",
    "\n",
    "Hi! I'm Abhinav! I am an entrepreneur and Vice President of Artificial Intelligence at Yarnit. I have spent over 15 years consulting and leadership roles in data science, machine learning and AI. My current focus is in the applied Generative AI domain focussing on solving enterprise needs through contextual intelligence. I'm passionate about AI advancements constantly exploring emerging technologies to push the boundaries and create positive impacts in the world. Let’s build the future, together!\n",
    "\n",
    "[If you haven't already, get your copy of A Simple Guide to Retrieval Augmented Generation here](https://mng.bz/6ePo)\n",
    "\n",
    "USE CODE ```OUkimothi``` for a 45% discount!\n",
    "\n",
    "<a href=\"https://mng.bz/6ePo\" target=\"_blank\">\n",
    "    <img src=\"./Assets/Images/NewMEAPFooter.png\" alt=\"New MEAP\" width= 70%\" />\n",
    "</a>\n",
    "\n",
    "#### If you'd like to chat, I'd be very happy to connect\n",
    "\n",
    "[![GitHub followers](https://img.shields.io/badge/Github-000000?style=for-the-badge&logo=github&logoColor=black&color=orange)](https://github.com/abhinav-kimothi)\n",
    "[![LinkedIn](https://img.shields.io/badge/LinkedIn-000000?style=for-the-badge&logo=linkedin&logoColor=orange&color=black)](https://www.linkedin.com/comm/mynetwork/discovery-see-all?usecase=PEOPLE_FOLLOWS&followMember=abhinav-kimothi)\n",
    "[![Medium](https://img.shields.io/badge/Medium-000000?style=for-the-badge&logo=medium&logoColor=black&color=orange)](https://medium.com/@abhinavkimothi)\n",
    "[![Insta](https://img.shields.io/badge/Instagram-000000?style=for-the-badge&logo=instagram&logoColor=orange&color=black)](https://www.instagram.com/akaiworks/)\n",
    "[![Mail](https://img.shields.io/badge/email-000000?style=for-the-badge&logo=gmail&logoColor=black&color=orange)](mailto:abhinav.kimothi.ds@gmail.com)\n",
    "[![X](https://img.shields.io/badge/Follow-000000?style=for-the-badge&logo=X&logoColor=orange&color=black)](https://twitter.com/abhinav_kimothi)\n",
    "[![Linktree](https://img.shields.io/badge/Linktree-000000?style=for-the-badge&logo=linktree&logoColor=black&color=orange)](https://linktr.ee/abhinavkimothi)\n",
    "[![Gumroad](https://img.shields.io/badge/Gumroad-000000?style=for-the-badge&logo=gumroad&logoColor=orange&color=black)](https://abhinavkimothi.gumroad.com/)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
